---
title: "snRNAseq_QC_Doublet_Cleanup_v1"
author: "Katie Prater"
date: "06/19/2020"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
Sys.setenv(PATH = "/home/src/python-3.8.1/bin:$PATH:")
library(stringr)
library(Seurat)
library(ggplot2)
library(Matrix)
library(filesstrings)
library(stringr)
library(rlist)
```
#Jayadev Lab snRNAseq Pipeline

##Doublet Detection for QC cleanup of data

### R Markdown note:
This is an R Markdown document. When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.

##First ensure you have the variables and directories set up as needed for the project. This should be identical to the code chunk you edited the first time. ***You DO need to edit this code chunk! ***
```{r Directory_setup}
#Document your code:
print("This is when this code was started:")
print(Sys.time())

#Define your project folder:
#projdir <- "path_to_my_project_folder_name"
projdir <- "~/jayadev/PU.1/exp_2-25-20/Full_sequence/code/pu1_only/"

#Name of the Rdata file to load in:
start_data <- "soupX_ws_pu1_only.rdata"


#__________________Do not edit below this line______________________________

```

## Load the Seurat data object you need
Load the data from the previous code chunk (but before thresholding and normalization were performed)
```{r Load_Data}
#Load the thresholded Seurat object saved before
data_loc<-file.path(projdir, "code", start_data, fsep = "/")
print(paste('Loading data', start_data, sep = " "))
load(start_data)

#Number of participants/samples to process:
numsubs <- length(samples)
print(paste(numsubs,'samples processing.', fsep = " "))

#Ensure paths are set for code:
outpath <- file.path(projdir, outdir, "doublet_detection", fsep = "/")
dir.create(outpath)

```

```{r Generate_matrices}
#Split the Seurat object back into its separate participant pieces so that we can run the cleanup steps on each individual.
print('Splitting Seurat Object to run Doublet Detection on Individuals')
ss_data_split <- SplitObject(ss_data_decon, split.by = "orig.ident") 

#Generate a counts matrix for each sample
#Split out the count matrices as sparse matrices named by the sample
cm_name <- rep(NA, length(samples))
ss_obj_names <- list.names(ss_data_split)
for (index in 1:length(samples)) {
  cm_name[index]<-paste0('count_matrix_', names[index], collapse = "")
  print(paste('Working on matrix:', cm_name[index], sep=" "))
  ss_obj_index<-match(samples[index], ss_obj_names)
  assign(cm_name[index],GetAssayData(object = ss_data_split[[ss_obj_index]], slot = "data"))
  print(paste('Matrices match dimensions:', identical(dim(get(cm_name[index])), dim(ss_data_split[[ss_obj_index]]))))
}

#Generate a list of genes
genes<-row.names(get(cm_name[index]))

#Transpose the sparse matrices for Scrublet (cells as rows and genes as columns)
tcm_name<- rep(NA, length(samples))
for (index in 1:length(samples)) {
  tcm_name[index]<-paste0('tcm_', names[index], collapse = "")
  print(paste('Transposing matrix:', tcm_name[index], sep=" "))
  assign(tcm_name[index],t(get(cm_name[index])))
  print(paste('Matrix properly transposed:',identical(ncol(get(cm_name[index])), nrow(get(tcm_name[index]))) ,sep=' '))
}

#Remove objects that are no longer needed
rm(ss_data_decon, ss_data_split)
objs<-ls()
objects_to_remove<-c(grep('count_', objs, value = TRUE))
rm(list=objects_to_remove)

save.image(file = paste0("decon_split_matrices_ws", suffix, ".rdata"), compress = TRUE)
```

## Do doublet detection using Scrublet:
### Scrublet is a python package, so we are using a python code chunk here for a bit.

```{python Scrublet_Detection}
#Document which version of Python you are using
import sys
print("You are using Python {}.{}.".format(sys.version_info.major,  sys.version_info.minor))

# Setup Python with the libraries we want to use
import scrublet as scr #(for R: scr<-import('scrublet'))
import scipy.io
import matplotlib.pyplot as plt
import numpy as np
import os
import csv

#Run Scrublet on all samples
samples = list(r.tcm_name)
for sample in samples:

    #Read in the transposed counts matrix to scrublet and python
    #Matrix needs to be cells as rows and genes as columns
    input_dir = r.projdir
    counts_matrix = r[sample]
    genes = r.genes

    #Print to screen the size of the matrix so we can see the number of genes
    print('Counts matrix shape: {} rows, {} columns'.format(counts_matrix.shape[0], counts_matrix.shape[1]))
    print('Number of genes in gene list: {}'.format(len(genes)))

    #Initialize the scrublet object
    scrub = scr.Scrublet(counts_matrix, expected_doublet_rate=0.12)
    #Expected doublet rate is 0.8% per 1600 cells based on 10X Chromium documentation.       #Kevin loads ~24000 nuclei, so we expect our doublet rate to be about 12%
    print('Expected Doublet Rate is: 0.12')

    #Run default Scrublet pipeline
    print('Running default Scrublet pipeline')
    #import time
    #t = time.localtime()
    #current_time = time.strftime("%H:%M:%S", t)
    #print(current_time)
    doublet_scores, predicted_doublets = scrub.scrub_doublets(
      min_counts=2, 
      min_cells=3,
      min_gene_variability_pctl=85,
      n_prin_comps=30
    )
    
  
    #Write the doublet_scores and predicted_doublets to a csv file for each sample
    print('Doublet scores shape: {} rows'.format(doublet_scores.shape[0])) 
    print('Predicted Doublets shape: {} rows'.format(predicted_doublets.shape[0]))
    with open(sample[4:]+'_decon_doublet_scores.csv', 'w', newline='\n') as csvfile:
        samplewriter = csv.writer(
            csvfile,
            delimiter=',',
            quotechar='|',
            quoting=csv.QUOTE_MINIMAL
        )
        samplewriter.writerow([
            'doublet_scores',
            'predicted_doublets',
            'threshold_is_{}'.format(scrub.threshold_)
        ])
        for doublet_index, doublet_score in enumerate(doublet_scores):
            samplewriter.writerow([
                doublet_score,
                predicted_doublets[doublet_index]
            ])
     
    #Visualize the data and the threshold Scrublet chose for doublets
    scrub.plot_histogram();
    plt.suptitle(sample[4:]+' threshold is {}'.format(scrub.threshold_),fontweight='bold')
    plt.show()
    plt.savefig(sample[4:] + '_decon_doublet_histo.png', transparent=True)
    
    #Run UMAP to visualize doublets in clusters
    print('Running UMAP...')
    scrub.set_embedding('UMAP', scr.get_umap(scrub.manifold_obs_, 10, min_dist=0.3))
    print('Done.')
    
    #Plot UMAPS. Doublets should cluster together.
    scrub.plot_embedding('UMAP', order_points=True);
    plt.suptitle(sample[4:])
    plt.show()
    plt.savefig(sample[4:] + '_decon_doublet_umaps.png', transparent=True)
```

##Scrublet has been run, now we put all the generated data in the output folder.
```{r Cleanup_Files}
#Move the histograms and umap images to the output folder.
pngs<- Sys.glob("*.png")
move_files(pngs, outpath, overwrite = FALSE)

#Move the csv files to the output folder
csvs<- Sys.glob("*scores.csv")
move_files(csvs, outpath, overwrite = FALSE)

print(paste('Scrublet data and plots stored in:', outpath, sep = ' '))
```

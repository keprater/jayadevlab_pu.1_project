---
title: "snRNAseq_Remove_Doublets_v1"
author: "Katie Prater and Kevin Green"
date: "5/29/2020"
output:
  html_document:
    df_print: paged
params:
  container: ""
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
library(Seurat)
library(ggplot2)
library(Matrix)
library(filesstrings)
```
# Jayadev Lab snRNAseq Pipeline

## Doublet Detection for QC cleanup of data

### R Markdown note:
This is an R Markdown document. When you click the **Knit** button a document
will be generated that includes both content as well as the output of any
embedded R code chunks within the document.

## Set samples that need to be rerun and the doublet thresholds that you want.
# You MUST edit this for your dataset to run properly!!!!
```{r Directory_setup}
# Name of the Rdata file to load in:
start_data <- "decon_split_matrices_ws_20210331.rdata"

# Define the .csv file name of the sample variables to be added for analysis
# (e.g. Pt_ID, pathology, diagnosis, age, etc.)
# THIS MUST BE IN THE DATA FOLDER
# YOU MUST HAVE "Pt_ID" AND AT LEAST ONE OTHER VARIABLE IN THE METADATA
# THE FIRST ROW MUST BE THE VARIABLE NAMES
csv <- "pu1_22samples_metadata.csv"

#__________________Do not edit below this line______________________________
```

### Load in the data you specified:
```{r Load_Data}
# Document your code:
print(paste("This is when this code was started:", Sys.time()))

# Print which container is being used
if (identical(params$container, "")) {
  stop("The container number must be documented before rendering.
       params= list(container = ###)")
}
print(paste("Container number", params$container, "was used"))

# Load the decontaminated Seurat object saved before
print(paste('Loading data', start_data, sep = " "))
load(paste0("../data/r_files/", start_data))

# Print container package versions, if changed from previous
if (!identical(params$container, prev_container)) {
  print("This code was run using:")
  print(sessionInfo())
  prev_container <- params$container
}
rm(params)
Sys.umask(mode = "000")

ss_csv <- csv
```

```{r Set_Directories}
# Number of participants/samples to process:
numsubs <- length(samples)
print(paste("Processing", numsubs, "samples", sep = " "))

# Documentation:
print(paste("This is where the project is:", normalizePath(projdir)))
print(paste("This is where the sample files are:", normalizePath(sample_dir)))
print(paste("This is where the data files will be saved:",
             normalizePath(outpath)))
```

### Remove the detected doublets from the count matrices
```{r Scrublet_Cleanup}
# Loop to apply Scrublet thresholds to the count matrices
scrub_tcm_name <- rep(NA, length(samples))
dub_score_name <- rep(NA, length(samples))
for (current_sample in 1:length(samples)) {
  # Pull the output from Scrublet back into R
  if (file.exists(paste0(doublet_path,'/', names[current_sample],
                        '_redo_decon_doublet_scores.csv'))) {
    data_filename <- paste0(names[current_sample],
                            '_redo_decon_doublet_scores.csv')
  } else {
    data_filename <- paste0(names[current_sample], '_decon_doublet_scores.csv')
  }
  scrub_out <- read.csv(paste0(doublet_path,'/', data_filename))
  doublets <- scrub_out[,2]

    # Count the number of doublets
  num_dubs <- length(doublets[doublets == 'True'])
    
  #Grab the doublet scores
  dub_score_name[[current_sample]] <- paste0(names[current_sample],
                                             '_dub_scores')
  assign(dub_score_name[current_sample], as.data.frame(scrub_out[,1]))
  assign(dub_score_name[current_sample],
         as.data.frame(get(dub_score_name[current_sample])[doublets == 'False',]))
  print(paste(tcm_name[current_sample],
              'Doublet scores retrieved for non-doublets:',
              identical(nrow(get(dub_score_name[current_sample])),
                        nrow(get(tcm_name[current_sample]))-num_dubs)))

  # Remove doublets from data
  scrub_tcm_name[[current_sample]] <- paste0(tcm_name[current_sample],
                                             '_scrubbed_data')
  assign(scrub_tcm_name[current_sample],
         get(tcm_name[current_sample])[doublets == 'False',])
  print(paste(tcm_name[current_sample], num_dubs, 'Doublets removed properly:',
              identical(nrow(get(scrub_tcm_name[current_sample])),
                        nrow(get(tcm_name[current_sample]))-num_dubs)))
}

# Transpose the sparse matrices to go back to Seurat (genes as rows and cells 
# as columns)
scrub_cm_name <- rep(NA, length(samples))
for (index in 1:length(samples)) {
  scrub_cm_name[index] <- paste0('scrub_cm_', names[index], collapse = "")
  print(paste('Transposing matrix:', scrub_tcm_name[index], sep=" "))
  assign(scrub_cm_name[index], t(get(scrub_tcm_name[index])))
  print(paste('Matrix properly transposed:',
              identical(ncol(get(scrub_cm_name[index])),
                        nrow(get(scrub_tcm_name[index]))), sep=' '))
}

# Remove extraneous variables from the workspace
objs <- ls()
objects_to_remove <- c('scrub_out', 'doublets', grep('tcm_', objs, value = TRUE))
rm(list = objects_to_remove)
rm(current_sample, data_filename, file, genes, index, num_dubs,
   objects_to_remove, objs)

# Save the workspace to load later
save.image(file = paste0(rfiles_path, "decon_scrubbed_split_matrices_ws",
                         suffix, ".rdata"),
           compress = TRUE)
```

### Load the matrices with doublets removed back into Seurat
```{r Seurat_Objects}
# Read in meta data from csv file
print(paste("Adding metadata from ../data/", ss_csv))
meta_data <- read.csv(paste0("../data/", ss_csv), header = TRUE)

# Generate a Seurat object out of the scrubbed matrices and add meta data
print('Generating Seurat Objects from scrubbed matrices')
for (file in 1:length(names)) {
  sample_meta_data <- meta_data[file,]
  sample_meta_data <- sample_meta_data[rep(seq_len(nrow(sample_meta_data)),
                                      each = ncol(get(scrub_cm_name[file]))), ]
  row.names(sample_meta_data) <- get(scrub_cm_name[file])@Dimnames[[2]]
  sample_meta_data <- cbind(sample_meta_data, get(dub_score_name[file]))
  colnames(sample_meta_data)[ncol(meta_data) + 1] <- 'Dub_scores'
  cat("Creating", names[[file]], "Seurat object...\n")
  ss_data_scrubbed <- CreateSeuratObject(counts = get(scrub_cm_name[file]),
                                         min.cells = 1,
                                         project = samples[file],
                                         meta.data = sample_meta_data)
  assign(x=names[file], ss_data_scrubbed)
  print(paste(names[[file]], 'seurat object properly created:',
              identical(ncol(ss_data_scrubbed),
                        ncol(get(scrub_cm_name[[file]])))))
  rm(list=scrub_cm_name[[file]], sample_meta_data)
}

rm(meta_data, scrub_cm_name, cm_name)

#Combine the seurat objects back to one matrix
print("Aggregating the Seurat objects...")
if (numsubs > 1) {
  ss_data_scrubbed <- merge(get(names[1]), get(names[2]))
  rm(list = c(names[[1]], names[[2]]))
  if (length(samples) >= 3){
    for (obj in 3:length(samples)){
      ss_data_scrubbed <- merge(ss_data_scrubbed, get(names[obj]))
      rm(list = names[[obj]])
    }
    rm(obj)
  }
} else {
  rm(list = names)
}
ss_data_scrubbed[["percent.mito"]] <- PercentageFeatureSet(ss_data_scrubbed,
                                                           pattern = "^MT-")

# Checks if the number of samples expected match the dimensions of your data
if (numsubs == length(unique(ss_data_scrubbed@meta.data$Pt_ID))){
  cat("The number of samples in your Seurat object matches the", 
        "number of expected samples")
} else {
  warning("The number of samples in your Seurat object does not ",
          "match the number of expected samples")
}

# Remove extraneous variables from the workspace
objs <- ls()
objects_to_remove <- c(grep('dub_', objs, value = TRUE))
rm(list = objects_to_remove)
rm(file, objects_to_remove, objs, doublet_path)

# Save the Seurat object so that we can load it again later
print(paste0("Saving object as seurat_ss_data_decon_scrubbed_thresholded",
                      suffix, '.rds'))
saveRDS(ss_data_scrubbed,
        file = paste0(rfiles_path, "seurat_ss_data_decon_scrubbed_thresholded",
                      suffix, '.rds'))
print("Saving the workspace...")
save.image(file = paste0(rfiles_path, "decon_scrubbed_thresholded_ws", suffix,
                         ".rdata"), compress = TRUE)
```
